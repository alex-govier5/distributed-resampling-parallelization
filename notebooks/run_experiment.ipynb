{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7de5e57c-da3a-4f17-ab8b-370b8a7b8c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "os.chdir('C:\\\\Users\\\\Owner\\\\Documents\\\\School\\\\University Year 4\\\\Semester 2\\\\Honours Project\\\\distributed-resampling-parallelization')  # Move up to the parent directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0612d809-bd76-4114-90f0-fab211a70f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "DATA_RAW_DIR = f\"{DATA_DIR}/raw\"\n",
    "DATA_PROCESSED_DIR = f\"{DATA_DIR}/processed\"\n",
    "\n",
    "RESULT_DIR = \"results\"\n",
    "RESULT_EXECUTION_TIME_DIR = f\"{RESULT_DIR}\"\n",
    "RESULT_PREDICTIVE_PERFORMANCE_DIR = f\"{RESULT_DIR}/predictive_performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c542890-ed1a-4fba-8580-d2e1cc1a3090",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = {\n",
    "    # \"boston\": \"HousValue\",\n",
    "    # \"Abalone\": \"Rings\",\n",
    "    # \"bank8FM\": \"rej\",\n",
    "    # \"heat\": \"heat\",\n",
    "    # \"cpuSm\": \"usr\",\n",
    "    # \"energy\": \"Appliances\",\n",
    "    # \"superconductivity\": \"critical_temp\"\n",
    "}\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    \"base\": {\n",
    "        \"name\": \"No Sampling\",\n",
    "        \"file_postfix\": \"\"\n",
    "    },\n",
    "    \"rus\": {\n",
    "        \"name\": \"RUS\",\n",
    "        \"file_postfix\": \"_rus\"\n",
    "    },\n",
    "    \"ros\": {\n",
    "        \"name\": \"ROS\",\n",
    "        \"file_postfix\": \"_ros\"\n",
    "    },\n",
    "    \"smogn\": {\n",
    "        \"name\": \"SMOGN\",\n",
    "        \"file_postfix\": \"_smogn\"\n",
    "    },\n",
    "    \"dist_smogn_2\": {\n",
    "        \"name\": \"Distributed SMOGN (k_partitions = 2)\",\n",
    "        \"file_postfix\": \"_dist_smogn_2\"\n",
    "    },\n",
    "    \"dist_smogn_4\": {\n",
    "        \"name\": \"Distributed SMOGN (k_partitions = 4)\",\n",
    "        \"file_postfix\": \"_dist_smogn_4\"\n",
    "    },\n",
    "    \"dist_smogn_8\": {\n",
    "        \"name\": \"Distributed SMOGN (k_partitions = 8)\",\n",
    "        \"file_postfix\": \"_dist_smogn_8\"\n",
    "    }\n",
    "}\n",
    "\n",
    "REGRESSORS = {\n",
    "    \"lr\": {\n",
    "        \"name\": \"Linear Regression (LR)\",\n",
    "        \"variants\": [\n",
    "            LinearRegression()\n",
    "        ]\n",
    "    },\n",
    "    \"svm\": {\n",
    "        \"name\": \"Support Vector Machine (SVM)\",\n",
    "        \"variants\": [\n",
    "            SVR(C=10, gamma=0.01),\n",
    "            SVR(C=10, gamma=0.001),\n",
    "            SVR(C=150, gamma=0.01),\n",
    "            SVR(C=150, gamma=0.001),\n",
    "            SVR(C=300, gamma=0.01),\n",
    "            SVR(C=300, gamma=0.001)\n",
    "        ]\n",
    "    },\n",
    "    \"rf\": {\n",
    "        \"name\": \"Random Forest (RF)\",\n",
    "        \"variants\": [\n",
    "            RandomForestRegressor(min_samples_leaf=1, min_samples_split=2),\n",
    "            RandomForestRegressor(min_samples_leaf=1, min_samples_split=5),\n",
    "            RandomForestRegressor(min_samples_leaf=2, min_samples_split=2),\n",
    "            RandomForestRegressor(min_samples_leaf=2, min_samples_split=5),\n",
    "            RandomForestRegressor(min_samples_leaf=4, min_samples_split=2),\n",
    "            RandomForestRegressor(min_samples_leaf=4, min_samples_split=5)\n",
    "        ]\n",
    "    },\n",
    "    \"nn\": {\n",
    "        \"name\": \"Neural Network (NN)\",\n",
    "        \"variants\": [\n",
    "            MLPRegressor(hidden_layer_sizes=1, max_iter=500),\n",
    "            MLPRegressor(hidden_layer_sizes=1, max_iter=1000),\n",
    "            MLPRegressor(hidden_layer_sizes=5, max_iter=500),\n",
    "            MLPRegressor(hidden_layer_sizes=5, max_iter=1000),\n",
    "            MLPRegressor(hidden_layer_sizes=10, max_iter=500),\n",
    "            MLPRegressor(hidden_layer_sizes=10, max_iter=1000)\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10dded7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m experiment, experiment_config \u001b[38;5;129;01min\u001b[39;00m EXPERIMENTS\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m----> 9\u001b[0m     train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATA_PROCESSED_TRAIN_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/sales\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mexperiment_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile_postfix\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     columns_to_drop \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSerial Number\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate Recorded\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAddress\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNon Use Code\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAssessor Remarks\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPM remarks\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m     train \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mcolumns_to_drop)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1012\u001b[0m     dialect,\n\u001b[0;32m   1013\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[0;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    615\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    617\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 618\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1896\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1895\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1898\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:581\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "# Code for sales.csv e02d06bb28a2d42907f8898c619cf73c7aa1f335\n",
    "DATA_PROCESSED_TRAIN_DIR = f\"{DATA_PROCESSED_DIR}/sales/train\"\n",
    "DATA_PROCESSED_TEST_DIR = f\"{DATA_PROCESSED_DIR}/sales/test\"\n",
    "\n",
    "for regressor, regressor_config in REGRESSORS.items():\n",
    "    results = {}\n",
    "\n",
    "    for experiment, experiment_config in EXPERIMENTS.items():\n",
    "        train = pd.read_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/sales{experiment_config['file_postfix']}.csv\")\n",
    "        columns_to_drop = ['Serial Number', 'Date Recorded', 'Address', 'Non Use Code', 'Assessor Remarks', 'OPM remarks', 'Location']\n",
    "        train = train.drop(columns=columns_to_drop)\n",
    "\n",
    "        # Fill missing values for property and residential type with the mode (most frequent value)\n",
    "        train.loc[:, 'Property Type'] = train['Property Type'].fillna(train['Property Type'].mode()[0])\n",
    "        train.loc[:, 'Residential Type'] = train['Residential Type'].fillna(train['Residential Type'].mode()[0])\n",
    "\n",
    "        # One hot encoding\n",
    "        train = pd.get_dummies(train, columns=['Town', 'Property Type', 'Residential Type'])\n",
    "\n",
    "        # Apply power transformer for assessed value\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "        train['Assessed Value'] = pt.fit_transform(train[['Assessed Value']])\n",
    "\n",
    "        # Clip the values to remove outliers\n",
    "        lower_bound = train['Assessed Value'].quantile(0.01)\n",
    "        upper_bound = train['Assessed Value'].quantile(0.99)\n",
    "        train['Assessed Value'] = train['Assessed Value'].clip(lower_bound, upper_bound)\n",
    "\n",
    "        # Apply power transformer for sale amount\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "        train['Sale Amount'] = pt.fit_transform(train[['Sale Amount']])\n",
    "\n",
    "        # Clip the values to remove outliers\n",
    "        lower_bound = train['Sale Amount'].quantile(0.01)\n",
    "        upper_bound = train['Sale Amount'].quantile(0.99)\n",
    "        train['Sale Amount'] = train['Sale Amount'].clip(lower_bound, upper_bound)\n",
    "\n",
    "        # Apply power transformer to sales ratio\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "        train['Sales Ratio'] = pt.fit_transform(train[['Sales Ratio']])\n",
    "\n",
    "        # Clip the values to remove outliers\n",
    "        lower_bound = train['Sales Ratio'].quantile(0.01)\n",
    "        upper_bound = train['Sales Ratio'].quantile(0.99)\n",
    "        train['Sales Ratio'] = train['Sales Ratio'].clip(lower_bound, upper_bound)\n",
    "\n",
    "        test = pd.read_csv(f\"{DATA_PROCESSED_TEST_DIR}/sales.csv\")\n",
    "        columns_to_drop = ['Serial Number', 'Date Recorded', 'Address', 'Non Use Code', 'Assessor Remarks', 'OPM remarks', 'Location']\n",
    "        test = test.drop(columns=columns_to_drop)\n",
    "\n",
    "        # Fill missing values for property and residential type with the mode (most frequent value)\n",
    "        test.loc[:, 'Property Type'] = test['Property Type'].fillna(test['Property Type'].mode()[0])\n",
    "        test.loc[:, 'Residential Type'] = test['Residential Type'].fillna(test['Residential Type'].mode()[0])\n",
    "\n",
    "        # One hot encoding\n",
    "        test = pd.get_dummies(test, columns=['Town', 'Property Type', 'Residential Type'])\n",
    "        test = test.reindex(columns=train.columns, fill_value=0)\n",
    "\n",
    "        # Apply power transformer for assessed value\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "        test['Assessed Value'] = pt.fit_transform(test[['Assessed Value']])\n",
    "\n",
    "        # Clip the values to remove outliers\n",
    "        lower_bound = test['Assessed Value'].quantile(0.01)\n",
    "        upper_bound = test['Assessed Value'].quantile(0.99)\n",
    "        test['Assessed Value'] = test['Assessed Value'].clip(lower_bound, upper_bound)\n",
    "\n",
    "        # Apply power transformer for sale amount\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "        test['Sale Amount'] = pt.fit_transform(test[['Sale Amount']])\n",
    "\n",
    "        # Clip the values to remove outliers\n",
    "        lower_bound = test['Sale Amount'].quantile(0.01)\n",
    "        upper_bound = test['Sale Amount'].quantile(0.99)\n",
    "        test['Sale Amount'] = test['Sale Amount'].clip(lower_bound, upper_bound)\n",
    "\n",
    "        # Apply power transformer to sales ratio\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "        test['Sales Ratio'] = pt.fit_transform(test[['Sales Ratio']])\n",
    "\n",
    "        # Clip the values to remove outliers\n",
    "        lower_bound = test['Sales Ratio'].quantile(0.01)\n",
    "        upper_bound = test['Sales Ratio'].quantile(0.99)\n",
    "        test['Sales Ratio'] = test['Sales Ratio'].clip(lower_bound, upper_bound)\n",
    "\n",
    "        y_train = train.pop('Sale Amount')\n",
    "        x_train = train\n",
    "\n",
    "        y_test = test.pop('Sale Amount')\n",
    "        x_test = test\n",
    "\n",
    "        numeric_cols = ['Assessed Value', 'Sales Ratio']  # List of numeric columns to scale\n",
    "        one_hot_cols_train = [col for col in x_train.columns if col not in numeric_cols]\n",
    "        one_hot_cols_test = [col for col in x_test.columns if col not in numeric_cols]\n",
    "        # Apply MinMaxScaler to only the numeric columns\n",
    "        scaler = MinMaxScaler().fit(pd.concat([x_train[numeric_cols], x_test[numeric_cols]]))\n",
    "\n",
    "        x_train[numeric_cols] = scaler.transform(x_train[numeric_cols])\n",
    "        x_test[numeric_cols] = scaler.transform(x_test[numeric_cols])\n",
    "\n",
    "        x_train = pd.concat([x_train[numeric_cols], x_train[one_hot_cols_train]], axis=1)\n",
    "        x_test = pd.concat([x_test[numeric_cols], x_test[one_hot_cols_test]], axis=1)\n",
    "\n",
    "        y_phi = pd.read_csv(f\"{DATA_PROCESSED_TEST_DIR}/sales_phi.csv\")\n",
    "\n",
    "        mae_list = []\n",
    "        rmse_list = []\n",
    "\n",
    "        results[experiment_config[\"name\"]] = {}\n",
    "\n",
    "        for model in regressor_config[\"variants\"]:\n",
    "            model.fit(x_train, y_train)\n",
    "\n",
    "            y_pred = model.predict(x_test)\n",
    "\n",
    "            mae_list.append(mean_absolute_error(y_true=y_test, y_pred=y_pred, sample_weight=y_phi))\n",
    "            rmse_list.append(mean_squared_error(y_true=y_test, y_pred=y_pred, sample_weight=y_phi, squared=False))\n",
    "\n",
    "        results[experiment_config[\"name\"]][\"mae\"] = round(np.mean(mae_list), 3)\n",
    "        results[experiment_config[\"name\"]][\"rmse\"] = round(np.mean(rmse_list), 3)\n",
    "\n",
    "    pd.DataFrame(data=results).transpose().to_csv(f\"{RESULT_PREDICTIVE_PERFORMANCE_DIR}/sales/{regressor}.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ca4fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, label_col in DATASETS.items():\n",
    "    DATA_PROCESSED_TRAIN_DIR = f\"{DATA_PROCESSED_DIR}/{dataset}/train\"\n",
    "    DATA_PROCESSED_TEST_DIR = f\"{DATA_PROCESSED_DIR}/{dataset}/test\"\n",
    "\n",
    "    for regressor, regressor_config in REGRESSORS.items():\n",
    "        results = {}\n",
    "\n",
    "        for experiment, experiment_config in EXPERIMENTS.items():\n",
    "            train = pd.read_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}{experiment_config['file_postfix']}.csv\")\n",
    "            y_train = train.pop(label_col)\n",
    "            x_train = pd.get_dummies(train)\n",
    "\n",
    "            test = pd.read_csv(f\"{DATA_PROCESSED_TEST_DIR}/{dataset}.csv\")\n",
    "            y_test = test.pop(label_col)\n",
    "            x_test = pd.get_dummies(test)\n",
    "\n",
    "            scaler = MinMaxScaler().fit(pd.concat([x_train, x_test]))\n",
    "\n",
    "            x_train = scaler.transform(x_train)\n",
    "            x_test = scaler.transform(x_test)\n",
    "\n",
    "            y_phi = pd.read_csv(f\"{DATA_PROCESSED_TEST_DIR}/{dataset}_phi.csv\")\n",
    "\n",
    "            mae_list = []\n",
    "            rmse_list = []\n",
    "\n",
    "            results[experiment_config[\"name\"]] = {}\n",
    "\n",
    "            for model in regressor_config[\"variants\"]:\n",
    "                model.fit(x_train, y_train)\n",
    "\n",
    "                y_pred = model.predict(x_test)\n",
    "\n",
    "                mae_list.append(mean_absolute_error(y_true=y_test, y_pred=y_pred, sample_weight=y_phi))\n",
    "                rmse_list.append(mean_squared_error(y_true=y_test, y_pred=y_pred, sample_weight=y_phi, squared=False))\n",
    "\n",
    "            results[experiment_config[\"name\"]][\"mae\"] = round(np.mean(mae_list), 3)\n",
    "            results[experiment_config[\"name\"]][\"rmse\"] = round(np.mean(rmse_list), 3)\n",
    "\n",
    "        pd.DataFrame(data=results).transpose().to_csv(f\"{RESULT_PREDICTIVE_PERFORMANCE_DIR}/{dataset}/{regressor}.csv\",\n",
    "                                                      index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
