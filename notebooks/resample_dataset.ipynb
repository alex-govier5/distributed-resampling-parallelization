{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5b879b-5b69-47f4-9993-46ab31e7fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import cProfile\n",
    "from pyspark.sql import SparkSession\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "from src.relevance.phi import Phi\n",
    "from src.sampling.mixed_sampling.distributed_smogn import DistributedSMOGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c682212e-4ca2-4414-b60f-adac800c972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "DATA_RAW_DIR = f\"{DATA_DIR}/raw\"\n",
    "DATA_PROCESSED_DIR = f\"{DATA_DIR}/processed\"\n",
    "\n",
    "RESULT_DIR = \"results\"\n",
    "RESULT_EXECUTION_TIME_DIR = f\"{RESULT_DIR}\"\n",
    "RESULT_PREDICTIVE_PERFORMANCE_DIR = \"{RESULT_DIR}/predictive_performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "181a0fef-e87b-4584-8cd5-62b287c4d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = {\n",
    "    \"flights\": \"ActualElapsedTime\",\n",
    "    \"power\": \"Global_active_power\",\n",
    "    \"sales\": \"Sale Amount\"\n",
    "}\n",
    "\n",
    "def generateDF(dataset): \n",
    "    if dataset == \"flights\":\n",
    "        df = pd.read_csv(f\"{DATA_RAW_DIR}/flights.csv\", usecols=[\"ActualElapsedTime\", \"CRSElapsedTime\", \"DepDelayMinutes\", \"AirTime\", \"Distance\", \"TaxiOut\", \"TaxiIn\", \"DayOfWeek\", \"Month\", \"Quarter\"], nrows=1000)\n",
    "        df['DepDelayMinutes'].fillna(0, inplace=True)\n",
    "        df['AirTime'].fillna(df['AirTime'].median(), inplace=True)\n",
    "        df['ActualElapsedTime'].fillna(df['ActualElapsedTime'].median(), inplace=True)\n",
    "        df['TaxiOut'].fillna(df['TaxiOut'].median(), inplace=True)\n",
    "        df['TaxiIn'].fillna(df['TaxiIn'].median(), inplace=True)\n",
    "    elif dataset == \"power\":\n",
    "        df = pd.read_csv(f\"{DATA_RAW_DIR}/power.txt\", sep=';', usecols=[\"Global_active_power\", \"Global_reactive_power\", \"Voltage\", \"Global_intensity\", \"Sub_metering_1\", \"Sub_metering_2\", \"Sub_metering_3\"], na_values='?', nrows=1000)\n",
    "        df.fillna(df.mean(), inplace=True)\n",
    "    elif dataset == \"sales\":\n",
    "        df = pd.read_csv(f\"{DATA_RAW_DIR}/sales.csv\", usecols=[\"List Year\", \"Assessed Value\", \"Sale Amount\", \"Sales Ratio\", \"Property Type\", \"Residential Type\"], dtype={\"Property Type\": \"category\", \"Residential Type\": \"category\"}, nrows=1000)\n",
    "        df.loc[:, 'Property Type'] = df['Property Type'].fillna(df['Property Type'].mode()[0])\n",
    "        df.loc[:, 'Residential Type'] = df['Residential Type'].fillna(df['Residential Type'].mode()[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b18ad90c-487b-4848-af03-8ff4780b4250",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local[4]').appName('Distributed Resampling').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a65b79e0-d76d-4a2e-9b96-da039535548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_times = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75e54a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flights\n",
      "power\n",
      "sales\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for dataset, label_col in DATASETS.items():\n",
    "    DATA_PROCESSED_TRAIN_DIR = f\"{DATA_PROCESSED_DIR}/{dataset}/train\"\n",
    "    DATA_PROCESSED_TEST_DIR = f\"{DATA_PROCESSED_DIR}/{dataset}/test\"\n",
    "    spark.conf.set(\"spark.local.dir\", \"/new/temp/dir\")\n",
    "    df = generateDF(dataset)\n",
    "    \n",
    "    df = spark.createDataFrame(df)\n",
    "\n",
    "    relevance_col = \"phi\"\n",
    "    df = Phi(input_col=label_col, output_col=relevance_col).transform(df)\n",
    "\n",
    "    train, test = df.randomSplit(weights=[0.8, 0.2])\n",
    "    # train = train.sample(fraction=0.001)\n",
    "    train = train.drop(relevance_col)\n",
    "    test = test.toPandas()\n",
    "    phi = test.pop(relevance_col)\n",
    "\n",
    "    test.to_csv(f\"{DATA_PROCESSED_TEST_DIR}/{dataset}.csv\", index=False)\n",
    "    phi.to_csv(f\"{DATA_PROCESSED_TEST_DIR}/{dataset}_phi.csv\", index=False)\n",
    "\n",
    "    execution_times[dataset] = {}\n",
    "\n",
    "    train_base = train.toPandas()\n",
    "    train_base.to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}.csv\", index=False)\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_dist_smogn_2 = DistributedSMOGN(label_col=label_col, k_partitions=2).transform(train)\n",
    "    end_time = time.time()\n",
    "    execution_times[dataset][\"Distributed SMOGN (k_partitions = 2)\"] = round(end_time - start_time, 3)\n",
    "    train_dist_smogn_2.toPandas().to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}_dist_smogn_2.csv\", index=False)\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_dist_smogn_4 = DistributedSMOGN(label_col=label_col, k_partitions=4).transform(train)\n",
    "    end_time = time.time()\n",
    "    execution_times[dataset][\"Distributed SMOGN (k_partitions = 4)\"] = round(end_time - start_time, 3)\n",
    "    train_dist_smogn_4.toPandas().to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}_dist_smogn_4.csv\", index=False)\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_dist_smogn_8 = DistributedSMOGN(label_col=label_col, k_partitions=8).transform(train)\n",
    "    end_time = time.time()\n",
    "    execution_times[dataset][\"Distributed SMOGN (k_partitions = 8)\"] = round(end_time - start_time, 3)\n",
    "    train_dist_smogn_8.toPandas().to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}_dist_smogn_8.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64ba531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Resample sales dataset\n",
    "# def profile_cell():\n",
    "#     warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "#     DATA_PROCESSED_TRAIN_DIR = f\"{DATA_PROCESSED_DIR}/sales/train\"\n",
    "#     DATA_PROCESSED_TEST_DIR = f\"{DATA_PROCESSED_DIR}/sales/test\"\n",
    "#     spark.conf.set(\"spark.local.dir\", \"/new/temp/dir\")\n",
    "#     df = pd.read_csv(f\"{DATA_RAW_DIR}/sales.csv\", usecols=[\"List Year\", \"Assessed Value\", \"Sale Amount\", \"Sales Ratio\", \"Property Type\", \"Residential Type\"], dtype={\"Property Type\": \"category\", \"Residential Type\": \"category\"})\n",
    "#     df.loc[:, 'Property Type'] = df['Property Type'].fillna(df['Property Type'].mode()[0])\n",
    "#     df.loc[:, 'Residential Type'] = df['Residential Type'].fillna(df['Residential Type'].mode()[0])\n",
    "\n",
    "#     df = spark.createDataFrame(df)\n",
    "\n",
    "#     relevance_col = \"phi\"\n",
    "#     df = Phi(input_col=\"Sale Amount\", output_col=relevance_col).transform(df)\n",
    "\n",
    "#     train, test = df.randomSplit(weights=[0.8, 0.2])\n",
    "#     train = train.sample(fraction=0.01)\n",
    "#     train = train.drop(relevance_col)\n",
    "#     test = test.toPandas()\n",
    "#     phi = test.pop(relevance_col)\n",
    "\n",
    "#     test.to_csv(f\"{DATA_PROCESSED_TEST_DIR}/sales.csv\", index=False)\n",
    "#     phi.to_csv(f\"{DATA_PROCESSED_TEST_DIR}/sales_phi.csv\", index=False)\n",
    "\n",
    "#     execution_times[\"sales\"] = {}\n",
    "\n",
    "#     train_base = train.toPandas()\n",
    "#     train_base.to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/sales.csv\", index=False)\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     train_dist_smogn_2 = DistributedSMOGN(label_col=\"Sale Amount\", k_partitions=2).transform(train)\n",
    "#     end_time = time.time()\n",
    "#     execution_times[\"sales\"][\"Distributed SMOGN (k_partitions = 2)\"] = round(end_time - start_time, 3)\n",
    "#     train_dist_smogn_2.toPandas().to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/sales_dist_smogn_2.csv\", index=False)\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     train_dist_smogn_4 = DistributedSMOGN(label_col=\"Sale Amount\", k_partitions=4).transform(train)\n",
    "#     end_time = time.time()\n",
    "#     execution_times[\"sales\"][\"Distributed SMOGN (k_partitions = 4)\"] = round(end_time - start_time, 3)\n",
    "#     train_dist_smogn_4.toPandas().to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/sales_dist_smogn_4.csv\", index=False)\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     train_dist_smogn_8 = DistributedSMOGN(label_col=\"Sale Amount\", k_partitions=8).transform(train)\n",
    "#     end_time = time.time()\n",
    "#     execution_times[\"sales\"][\"Distributed SMOGN (k_partitions = 8)\"] = round(end_time - start_time, 3)\n",
    "#     train_dist_smogn_8.toPandas().to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/sales_dist_smogn_8.csv\", index=False)\n",
    "\n",
    "#     pass\n",
    "\n",
    "# cProfile.run('profile_cell()', 'profile_output.prof')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d2affa5-93d4-48b7-80cb-d7ffa9c91aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=execution_times).to_csv(f\"{RESULT_EXECUTION_TIME_DIR}/execution_time.csv\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
