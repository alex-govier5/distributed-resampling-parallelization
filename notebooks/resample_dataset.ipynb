{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a5b879b-5b69-47f4-9993-46ab31e7fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import cProfile\n",
    "from pyspark.sql import SparkSession\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "from src.relevance.phi import Phi\n",
    "from src.sampling.mixed_sampling.distributed_smogn import DistributedSMOGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c682212e-4ca2-4414-b60f-adac800c972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "DATA_RAW_DIR = f\"{DATA_DIR}/raw\"\n",
    "DATA_PROCESSED_DIR = f\"{DATA_DIR}/processed\"\n",
    "\n",
    "RESULT_DIR = \"results\"\n",
    "RESULT_EXECUTION_TIME_DIR = f\"{RESULT_DIR}\"\n",
    "RESULT_PREDICTIVE_PERFORMANCE_DIR = \"{RESULT_DIR}/predictive_performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "181a0fef-e87b-4584-8cd5-62b287c4d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = {\n",
    "    \"boston\": \"HousValue\",\n",
    "    \"Abalone\": \"Rings\",\n",
    "    \"bank8FM\": \"rej\",\n",
    "    \"heat\": \"heat\",\n",
    "    \"cpuSm\": \"usr\",\n",
    "    \"energy\": \"Appliances\",\n",
    "    \"superconductivity\": \"critical_temp\",\n",
    "    \"flights\": \"ActualElapsedTime\",\n",
    "    \"power\": \"Global_active_power\",\n",
    "    \"sales\": \"Sale Amount\"\n",
    "}\n",
    "\n",
    "def generateDF(dataset): \n",
    "    if dataset == \"flights\":\n",
    "        df = pd.read_csv(f\"{DATA_RAW_DIR}/flights.csv\", usecols=[\"ActualElapsedTime\", \"CRSElapsedTime\", \"DepDelayMinutes\", \"AirTime\", \"Distance\", \"TaxiOut\", \"TaxiIn\", \"DayOfWeek\", \"Month\", \"Quarter\"], nrows=1000)\n",
    "        df['DepDelayMinutes'].fillna(0, inplace=True)\n",
    "        df['AirTime'].fillna(df['AirTime'].median(), inplace=True)\n",
    "        df['ActualElapsedTime'].fillna(df['ActualElapsedTime'].median(), inplace=True)\n",
    "        df['TaxiOut'].fillna(df['TaxiOut'].median(), inplace=True)\n",
    "        df['TaxiIn'].fillna(df['TaxiIn'].median(), inplace=True)\n",
    "    elif dataset == \"power\":\n",
    "        df = pd.read_csv(f\"{DATA_RAW_DIR}/power.txt\", sep=';', usecols=[\"Global_active_power\", \"Global_reactive_power\", \"Voltage\", \"Global_intensity\", \"Sub_metering_1\", \"Sub_metering_2\", \"Sub_metering_3\"], na_values='?', nrows=1000)\n",
    "        df.fillna(df.mean(), inplace=True)\n",
    "    elif dataset == \"sales\":\n",
    "        df = pd.read_csv(f\"{DATA_RAW_DIR}/sales.csv\", usecols=[\"List Year\", \"Assessed Value\", \"Sale Amount\", \"Sales Ratio\", \"Property Type\", \"Residential Type\"], dtype={\"Property Type\": \"category\", \"Residential Type\": \"category\"}, nrows=1000)\n",
    "        df.loc[:, 'Property Type'] = df['Property Type'].fillna(df['Property Type'].mode()[0])\n",
    "        df.loc[:, 'Residential Type'] = df['Residential Type'].fillna(df['Residential Type'].mode()[0])\n",
    "    else:\n",
    "        df = pd.read_csv(f\"{DATA_RAW_DIR}/{dataset}.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b18ad90c-487b-4848-af03-8ff4780b4250",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName('Distributed Resampling')\n",
    "    .master('local[*]')  # Utilizes all available cores\n",
    "    .config('spark.executor.memory', '4g')  # Memory per executor\n",
    "    .config('spark.driver.memory', '4g')  # Memory for the driver\n",
    "    .config('spark.executor.cores', '2')  # Number of cores per executor\n",
    "    .config('spark.sql.shuffle.partitions', '200')  # Adjust shuffle partitions\n",
    "    .config('spark.storage.memoryFraction', '0.8')  # Allocate more memory for caching\n",
    "    .config('spark.network.timeout', '800s')  # Increase network timeout for large data\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a65b79e0-d76d-4a2e-9b96-da039535548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_times = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b75e54a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'data\\processed\\boston\\test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m test \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[0;32m     17\u001b[0m phi \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mpop(relevance_col)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATA_PROCESSED_TEST_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m phi\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PROCESSED_TEST_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_phi.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m execution_times[dataset] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistributed SMOGN (k_partitions = 2)\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m     23\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistributed SMOGN (k_partitions = 4)\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m     24\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistributed SMOGN (k_partitions = 8)\u001b[39m\u001b[38;5;124m\"\u001b[39m: []}\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:3961\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3950\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3952\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3953\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3954\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3958\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3959\u001b[0m )\n\u001b[1;32m-> 3961\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3964\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3966\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'data\\processed\\boston\\test'"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for dataset, label_col in DATASETS.items():\n",
    "    DATA_PROCESSED_TRAIN_DIR = f\"{DATA_PROCESSED_DIR}/{dataset}/train\"\n",
    "    DATA_PROCESSED_TEST_DIR = f\"{DATA_PROCESSED_DIR}/{dataset}/test\"\n",
    "    spark.conf.set(\"spark.local.dir\", \"/new/temp/dir\")\n",
    "    df = generateDF(dataset)\n",
    "    \n",
    "    df = spark.createDataFrame(df)\n",
    "\n",
    "    relevance_col = \"phi\"\n",
    "    df = Phi(input_col=label_col, output_col=relevance_col).transform(df)\n",
    "\n",
    "    train, test = df.randomSplit(weights=[0.8, 0.2])\n",
    "    train = train.drop(relevance_col)\n",
    "    test = test.toPandas()\n",
    "    phi = test.pop(relevance_col)\n",
    "\n",
    "    test.to_csv(f\"{DATA_PROCESSED_TEST_DIR}/{dataset}.csv\", index=False)\n",
    "    phi.to_csv(f\"{DATA_PROCESSED_TEST_DIR}/{dataset}_phi.csv\", index=False)\n",
    "\n",
    "    execution_times[dataset] = {\"Distributed SMOGN (k_partitions = 2)\": [],\n",
    "                                \"Distributed SMOGN (k_partitions = 4)\": [],\n",
    "                                \"Distributed SMOGN (k_partitions = 8)\": []}\n",
    "\n",
    "    train_base = train.toPandas()\n",
    "    train_base.to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}.csv\", index=False)\n",
    "    print(dataset)\n",
    "\n",
    "    for i in range(5):  # Perform 5 runs for each dataset\n",
    "        print(\"RUN: \"+i+1)\n",
    "        # k_partitions = 2\n",
    "        start_time = time.time()\n",
    "        train_dist_smogn_2 = DistributedSMOGN(label_col=label_col, k_partitions=2).transform(train)\n",
    "        end_time = time.time()\n",
    "        execution_times[dataset][\"Distributed SMOGN (k_partitions = 2)\"].append(end_time - start_time)\n",
    "        train_dist_smogn_2.toPandas().to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}_dist_smogn_2.csv\", index=False)\n",
    "\n",
    "        # k_partitions = 4\n",
    "        start_time = time.time()\n",
    "        train_dist_smogn_4 = DistributedSMOGN(label_col=label_col, k_partitions=4).transform(train)\n",
    "        end_time = time.time()\n",
    "        execution_times[dataset][\"Distributed SMOGN (k_partitions = 4)\"].append(end_time - start_time)\n",
    "        train_dist_smogn_4.toPandas().to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}_dist_smogn_4.csv\", index=False)\n",
    "\n",
    "        # k_partitions = 8\n",
    "        start_time = time.time()\n",
    "        train_dist_smogn_8 = DistributedSMOGN(label_col=label_col, k_partitions=8).transform(train)\n",
    "        end_time = time.time()\n",
    "        execution_times[dataset][\"Distributed SMOGN (k_partitions = 8)\"].append(end_time - start_time)\n",
    "        train_dist_smogn_8.toPandas().to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}_dist_smogn_8.csv\", index=False)\n",
    "\n",
    "    # Calculate average execution times for each configuration\n",
    "    execution_times[dataset] = {\n",
    "        config: round(sum(times) / len(times), 3)\n",
    "        for config, times in execution_times[dataset].items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Resample sales dataset\n",
    "# def profile_cell():\n",
    "#     warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "#     dataset = \"sales\"\n",
    "#     label_col = \"Sale Amount\"\n",
    "\n",
    "#     DATA_PROCESSED_TRAIN_DIR = f\"{DATA_PROCESSED_DIR}/{dataset}/train\"\n",
    "#     DATA_PROCESSED_TEST_DIR = f\"{DATA_PROCESSED_DIR}/{dataset}/test\"\n",
    "#     spark.conf.set(\"spark.local.dir\", \"/new/temp/dir\")\n",
    "#     df = pd.read_csv(f\"{DATA_RAW_DIR}/{dataset}.csv\", usecols=[\"List Year\", \"Assessed Value\", \"Sale Amount\", \"Sales Ratio\", \"Property Type\", \"Residential Type\"], dtype={\"Property Type\": \"category\", \"Residential Type\": \"category\"}, nrows=10000)\n",
    "#     df.loc[:, 'Property Type'] = df['Property Type'].fillna(df['Property Type'].mode()[0])\n",
    "#     df.loc[:, 'Residential Type'] = df['Residential Type'].fillna(df['Residential Type'].mode()[0])\n",
    "\n",
    "#     df = spark.createDataFrame(df)\n",
    "\n",
    "#     relevance_col = \"phi\"\n",
    "#     df = Phi(input_col=label_col, output_col=relevance_col).transform(df)\n",
    "\n",
    "#     train, test = df.randomSplit(weights=[0.8, 0.2])\n",
    "#     train = train.drop(relevance_col)\n",
    "#     test = test.toPandas()\n",
    "#     phi = test.pop(relevance_col)\n",
    "\n",
    "#     test.to_csv(f\"{DATA_PROCESSED_TEST_DIR}/{dataset}.csv\", index=False)\n",
    "#     phi.to_csv(f\"{DATA_PROCESSED_TEST_DIR}/{dataset}_phi.csv\", index=False)\n",
    "\n",
    "#     execution_times[dataset] = {\"Distributed SMOGN (k_partitions = 2)\": [],\n",
    "#                                 \"Distributed SMOGN (k_partitions = 4)\": [],\n",
    "#                                 \"Distributed SMOGN (k_partitions = 8)\": []}\n",
    "\n",
    "#     train_base = train.toPandas()\n",
    "#     train_base.to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}.csv\", index=False)\n",
    "\n",
    "#     for _ in range(5):  # Perform 5 runs for each dataset\n",
    "#         # k_partitions = 2\n",
    "#         start_time = time.time()\n",
    "#         train_dist_smogn_2 = DistributedSMOGN(label_col=label_col, k_partitions=2).transform(train)\n",
    "#         end_time = time.time()\n",
    "#         execution_times[dataset][\"Distributed SMOGN (k_partitions = 2)\"].append(end_time - start_time)\n",
    "#         train_dist_smogn_2.toPandas().to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}_dist_smogn_2.csv\", index=False)\n",
    "\n",
    "#         # k_partitions = 4\n",
    "#         start_time = time.time()\n",
    "#         train_dist_smogn_4 = DistributedSMOGN(label_col=label_col, k_partitions=4).transform(train)\n",
    "#         end_time = time.time()\n",
    "#         execution_times[dataset][\"Distributed SMOGN (k_partitions = 4)\"].append(end_time - start_time)\n",
    "#         train_dist_smogn_4.toPandas().to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}_dist_smogn_4.csv\", index=False)\n",
    "\n",
    "#         # k_partitions = 8\n",
    "#         start_time = time.time()\n",
    "#         train_dist_smogn_8 = DistributedSMOGN(label_col=label_col, k_partitions=8).transform(train)\n",
    "#         end_time = time.time()\n",
    "#         execution_times[dataset][\"Distributed SMOGN (k_partitions = 8)\"].append(end_time - start_time)\n",
    "#         train_dist_smogn_8.toPandas().to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}_dist_smogn_8.csv\", index=False)\n",
    "\n",
    "#     # Calculate average execution times for each configuration\n",
    "#     execution_times[dataset] = {\n",
    "#         config: round(sum(times) / len(times), 3)\n",
    "#         for config, times in execution_times[dataset].items()\n",
    "#     }\n",
    "\n",
    "#     pass\n",
    "\n",
    "# cProfile.run('profile_cell()', 'profile_output.prof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2affa5-93d4-48b7-80cb-d7ffa9c91aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=execution_times).to_csv(f\"{RESULT_EXECUTION_TIME_DIR}/execution_time.csv\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
