{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a5b879b-5b69-47f4-9993-46ab31e7fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import cProfile\n",
    "from pyspark.sql import SparkSession\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "from src.relevance.phi import Phi\n",
    "from src.sampling.mixed_sampling.distributed_smogn import DistributedSMOGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c682212e-4ca2-4414-b60f-adac800c972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "DATA_RAW_DIR = f\"{DATA_DIR}/raw\"\n",
    "DATA_PROCESSED_DIR = f\"{DATA_DIR}/processed\"\n",
    "\n",
    "RESULT_DIR = \"results\"\n",
    "RESULT_EXECUTION_TIME_DIR = f\"{RESULT_DIR}\"\n",
    "RESULT_PREDICTIVE_PERFORMANCE_DIR = \"{RESULT_DIR}/predictive_performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181a0fef-e87b-4584-8cd5-62b287c4d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = {\n",
    "    \"boston\": \"HousValue\",\n",
    "    \"Abalone\": \"Rings\",\n",
    "    \"bank8FM\": \"rej\",\n",
    "    \"heat\": \"heat\",\n",
    "    \"cpuSm\": \"usr\",\n",
    "    \"energy\": \"Appliances\",\n",
    "    \"superconductivity\": \"critical_temp\",\n",
    "    \"flights\": \"ActualElapsedTime\",\n",
    "    \"power\": \"Global_active_power\",\n",
    "    \"sales\": \"Sale Amount\"\n",
    "}\n",
    "\n",
    "def generateDF(dataset): \n",
    "    if dataset == \"flights\":\n",
    "        df = pd.read_csv(f\"{DATA_RAW_DIR}/flights.csv\", usecols=[\"ActualElapsedTime\", \"CRSElapsedTime\", \"DepDelayMinutes\", \"AirTime\", \"Distance\", \"TaxiOut\", \"TaxiIn\", \"DayOfWeek\", \"Month\", \"Quarter\"])\n",
    "        df['DepDelayMinutes'].fillna(0, inplace=True)\n",
    "        df['AirTime'].fillna(df['AirTime'].median(), inplace=True)\n",
    "        df['ActualElapsedTime'].fillna(df['ActualElapsedTime'].median(), inplace=True)\n",
    "        df['TaxiOut'].fillna(df['TaxiOut'].median(), inplace=True)\n",
    "        df['TaxiIn'].fillna(df['TaxiIn'].median(), inplace=True)\n",
    "    elif dataset == \"power\":\n",
    "        df = pd.read_csv(f\"{DATA_RAW_DIR}/power.txt\", sep=';', usecols=[\"Global_active_power\", \"Global_reactive_power\", \"Voltage\", \"Global_intensity\", \"Sub_metering_1\", \"Sub_metering_2\", \"Sub_metering_3\"], na_values='?')\n",
    "        df.fillna(df.mean(), inplace=True)\n",
    "    elif dataset == \"sales\":\n",
    "        df = pd.read_csv(f\"{DATA_RAW_DIR}/sales.csv\", usecols=[\"List Year\", \"Assessed Value\", \"Sale Amount\", \"Sales Ratio\", \"Property Type\", \"Residential Type\"], dtype={\"Property Type\": \"category\", \"Residential Type\": \"category\"})\n",
    "        df.loc[:, 'Property Type'] = df['Property Type'].fillna(df['Property Type'].mode()[0])\n",
    "        df.loc[:, 'Residential Type'] = df['Residential Type'].fillna(df['Residential Type'].mode()[0])\n",
    "    else:\n",
    "        df = pd.read_csv(f\"{DATA_RAW_DIR}/{dataset}.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b18ad90c-487b-4848-af03-8ff4780b4250",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName('Distributed Resampling')\n",
    "    .master('local[*]')  # Utilizes all available cores\n",
    "    .config('spark.executor.memory', '4g')  # Memory per executor\n",
    "    .config('spark.driver.memory', '4g')  # Memory for the driver\n",
    "    .config('spark.executor.cores', '2')  # Number of cores per executor\n",
    "    .config('spark.sql.shuffle.partitions', '200')  # Adjust shuffle partitions\n",
    "    .config('spark.storage.memoryFraction', '0.8')  # Allocate more memory for caching\n",
    "    .config('spark.network.timeout', '800s')  # Increase network timeout for large data\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a65b79e0-d76d-4a2e-9b96-da039535548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_times = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75e54a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boston\n",
      "RUN: 1\n",
      "RUN: 2\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for dataset, label_col in DATASETS.items():\n",
    "    DATA_PROCESSED_TRAIN_DIR = f\"{DATA_PROCESSED_DIR}/{dataset}/train\"\n",
    "    DATA_PROCESSED_TEST_DIR = f\"{DATA_PROCESSED_DIR}/{dataset}/test\"\n",
    "    spark.conf.set(\"spark.local.dir\", \"/new/temp/dir\")\n",
    "    df = generateDF(dataset)\n",
    "    \n",
    "    df = spark.createDataFrame(df)\n",
    "\n",
    "    relevance_col = \"phi\"\n",
    "    df = Phi(input_col=label_col, output_col=relevance_col).transform(df)\n",
    "\n",
    "    train, test = df.randomSplit(weights=[0.8, 0.2])\n",
    "    train = train.drop(relevance_col)\n",
    "    test = test.toPandas()\n",
    "    phi = test.pop(relevance_col)\n",
    "\n",
    "    test.to_csv(f\"{DATA_PROCESSED_TEST_DIR}/{dataset}.csv\", index=False)\n",
    "    phi.to_csv(f\"{DATA_PROCESSED_TEST_DIR}/{dataset}_phi.csv\", index=False)\n",
    "\n",
    "    execution_times[dataset] = {\"Distributed SMOGN (k_partitions = 2)\": [],\n",
    "                                \"Distributed SMOGN (k_partitions = 4)\": [],\n",
    "                                \"Distributed SMOGN (k_partitions = 8)\": []}\n",
    "\n",
    "    train_base = train.toPandas()\n",
    "    train_base.to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}.csv\", index=False)\n",
    "\n",
    "    for i in range(5):  # Perform 5 runs for each dataset\n",
    "        # k_partitions = 2\n",
    "        start_time = time.time()\n",
    "        train_dist_smogn_2 = DistributedSMOGN(label_col=label_col, k_partitions=2).transform(train)\n",
    "        end_time = time.time()\n",
    "        execution_times[dataset][\"Distributed SMOGN (k_partitions = 2)\"].append(end_time - start_time)\n",
    "        train_dist_smogn_2.toPandas().to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}_dist_smogn_2.csv\", index=False)\n",
    "\n",
    "        # k_partitions = 4\n",
    "        start_time = time.time()\n",
    "        train_dist_smogn_4 = DistributedSMOGN(label_col=label_col, k_partitions=4).transform(train)\n",
    "        end_time = time.time()\n",
    "        execution_times[dataset][\"Distributed SMOGN (k_partitions = 4)\"].append(end_time - start_time)\n",
    "        train_dist_smogn_4.toPandas().to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}_dist_smogn_4.csv\", index=False)\n",
    "\n",
    "        # k_partitions = 8\n",
    "        start_time = time.time()\n",
    "        train_dist_smogn_8 = DistributedSMOGN(label_col=label_col, k_partitions=8).transform(train)\n",
    "        end_time = time.time()\n",
    "        execution_times[dataset][\"Distributed SMOGN (k_partitions = 8)\"].append(end_time - start_time)\n",
    "        train_dist_smogn_8.toPandas().to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}_dist_smogn_8.csv\", index=False)\n",
    "\n",
    "    # Calculate average execution times for each configuration\n",
    "    execution_times[dataset] = {\n",
    "        config: round(sum(times) / len(times), 3)\n",
    "        for config, times in execution_times[dataset].items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Resample sales dataset\n",
    "# def profile_cell():\n",
    "#     warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "#     dataset = \"sales\"\n",
    "#     label_col = \"Sale Amount\"\n",
    "\n",
    "#     DATA_PROCESSED_TRAIN_DIR = f\"{DATA_PROCESSED_DIR}/{dataset}/train\"\n",
    "#     DATA_PROCESSED_TEST_DIR = f\"{DATA_PROCESSED_DIR}/{dataset}/test\"\n",
    "#     spark.conf.set(\"spark.local.dir\", \"/new/temp/dir\")\n",
    "#     df = pd.read_csv(f\"{DATA_RAW_DIR}/{dataset}.csv\", usecols=[\"List Year\", \"Assessed Value\", \"Sale Amount\", \"Sales Ratio\", \"Property Type\", \"Residential Type\"], dtype={\"Property Type\": \"category\", \"Residential Type\": \"category\"}, nrows=10000)\n",
    "#     df.loc[:, 'Property Type'] = df['Property Type'].fillna(df['Property Type'].mode()[0])\n",
    "#     df.loc[:, 'Residential Type'] = df['Residential Type'].fillna(df['Residential Type'].mode()[0])\n",
    "\n",
    "#     df = spark.createDataFrame(df)\n",
    "\n",
    "#     relevance_col = \"phi\"\n",
    "#     df = Phi(input_col=label_col, output_col=relevance_col).transform(df)\n",
    "\n",
    "#     train, test = df.randomSplit(weights=[0.8, 0.2])\n",
    "#     train = train.drop(relevance_col)\n",
    "#     test = test.toPandas()\n",
    "#     phi = test.pop(relevance_col)\n",
    "\n",
    "#     test.to_csv(f\"{DATA_PROCESSED_TEST_DIR}/{dataset}.csv\", index=False)\n",
    "#     phi.to_csv(f\"{DATA_PROCESSED_TEST_DIR}/{dataset}_phi.csv\", index=False)\n",
    "\n",
    "#     execution_times[dataset] = {\"Distributed SMOGN (k_partitions = 2)\": [],\n",
    "#                                 \"Distributed SMOGN (k_partitions = 4)\": [],\n",
    "#                                 \"Distributed SMOGN (k_partitions = 8)\": []}\n",
    "\n",
    "#     train_base = train.toPandas()\n",
    "#     train_base.to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}.csv\", index=False)\n",
    "\n",
    "#     for _ in range(5):  # Perform 5 runs for each dataset\n",
    "#         # k_partitions = 2\n",
    "#         start_time = time.time()\n",
    "#         train_dist_smogn_2 = DistributedSMOGN(label_col=label_col, k_partitions=2).transform(train)\n",
    "#         end_time = time.time()\n",
    "#         execution_times[dataset][\"Distributed SMOGN (k_partitions = 2)\"].append(end_time - start_time)\n",
    "#         train_dist_smogn_2.toPandas().to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}_dist_smogn_2.csv\", index=False)\n",
    "\n",
    "#         # k_partitions = 4\n",
    "#         start_time = time.time()\n",
    "#         train_dist_smogn_4 = DistributedSMOGN(label_col=label_col, k_partitions=4).transform(train)\n",
    "#         end_time = time.time()\n",
    "#         execution_times[dataset][\"Distributed SMOGN (k_partitions = 4)\"].append(end_time - start_time)\n",
    "#         train_dist_smogn_4.toPandas().to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}_dist_smogn_4.csv\", index=False)\n",
    "\n",
    "#         # k_partitions = 8\n",
    "#         start_time = time.time()\n",
    "#         train_dist_smogn_8 = DistributedSMOGN(label_col=label_col, k_partitions=8).transform(train)\n",
    "#         end_time = time.time()\n",
    "#         execution_times[dataset][\"Distributed SMOGN (k_partitions = 8)\"].append(end_time - start_time)\n",
    "#         train_dist_smogn_8.toPandas().to_csv(f\"{DATA_PROCESSED_TRAIN_DIR}/{dataset}_dist_smogn_8.csv\", index=False)\n",
    "\n",
    "#     # Calculate average execution times for each configuration\n",
    "#     execution_times[dataset] = {\n",
    "#         config: round(sum(times) / len(times), 3)\n",
    "#         for config, times in execution_times[dataset].items()\n",
    "#     }\n",
    "\n",
    "#     pass\n",
    "\n",
    "# cProfile.run('profile_cell()', 'profile_output.prof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2affa5-93d4-48b7-80cb-d7ffa9c91aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=execution_times).to_csv(f\"{RESULT_EXECUTION_TIME_DIR}/execution_time.csv\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
